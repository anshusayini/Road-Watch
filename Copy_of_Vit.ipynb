{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHZaVmxxLtgZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pothole_dataset1_path = \"/content/drive/MyDrive/dataset/pothole1.zip\"\n",
        "pothole_dataset2_path = \"/content/drive/MyDrive/dataset/pothole2.zip\"\n",
        "pothole_dataset3_path = \"/content/drive/MyDrive/dataset/pothole3.zip\"\n",
        "crack_dataset_path = \"/content/drive/MyDrive/dataset/cracks.zip\"\n",
        "\n",
        "merged_pothole_dataset_path = \"/content/merged_pothole_dataset\"  # New folder in Colab\n",
        "binary_dataset_path = \"/content/binary_classification\"  # Final dataset folder\n",
        "\n"
      ],
      "metadata": {
        "id": "JWJnTOPwL0Fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/dataset/pothole1.zip\"  # Update with your actual path\n",
        "extract_path = \"/content/extracted_pothole1\"\n",
        "\n",
        "# Extract ZIP file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Extraction complete. Files are now in:\", extract_path)\n"
      ],
      "metadata": {
        "id": "i2_cMIyOL-Lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/dataset/pothole2.zip\"  # Update with your actual path\n",
        "extract_path = \"/content/extracted_pothole2\"\n",
        "\n",
        "# Extract ZIP file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Extraction complete. Files are now in:\", extract_path)\n"
      ],
      "metadata": {
        "id": "bylIDgVlL_5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/dataset/pothole3.zip\"  # Update with your actual path\n",
        "extract_path = \"/content/extracted_pothole3\"\n",
        "\n",
        "# Extract ZIP file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Extraction complete. Files are now in:\", extract_path)\n"
      ],
      "metadata": {
        "id": "nuY8718nMC9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"Files in Pothole Dataset 1:\", len(os.listdir(\"/content/extracted_pothole1/pothole_image_data/Pothole_Image_Data\")))\n",
        "print(\"Files in Pothole Dataset 2:\", len(os.listdir(\"/content/extracted_pothole2/images\")))\n",
        "print(\"Files in Pothole Dataset 3:\", len(os.listdir(\"/content/extracted_pothole3/dataset/dataset/classes/pothole/images\")))\n",
        "\n"
      ],
      "metadata": {
        "id": "6LbrKqI4MFkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "merged_pothole_dataset_path = \"/content/merged_pothole_dataset\"\n",
        "os.makedirs(merged_pothole_dataset_path, exist_ok=True)\n",
        "\n",
        "def merge_datasets(source_path, destination_path):\n",
        "    for file in os.listdir(source_path):\n",
        "        src = os.path.join(source_path, file)\n",
        "        dst = os.path.join(destination_path, file)\n",
        "        if os.path.isfile(src):\n",
        "            shutil.copy(src, dst)\n",
        "\n",
        "# Merge all three pothole datasets\n",
        "merge_datasets(\"/content/extracted_pothole1/pothole_image_data/Pothole_Image_Data\", merged_pothole_dataset_path)\n",
        "merge_datasets(\"/content/extracted_pothole2/images\", merged_pothole_dataset_path)\n",
        "merge_datasets(\"/content/extracted_pothole3/dataset/dataset/classes/pothole/images\", merged_pothole_dataset_path)\n",
        "\n",
        "print(\"Total Merged Pothole Images:\", len(os.listdir(merged_pothole_dataset_path)))\n",
        "\n"
      ],
      "metadata": {
        "id": "72s4UIW-MIOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/dataset/cracks.zip\"  # Update with actual path\n",
        "extract_path = \"/content/extracted_cracks\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Extraction complete. Files are now in:\", extract_path)\n"
      ],
      "metadata": {
        "id": "mGfdb7coMKTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "folder_path = '/content/merged_pothole_dataset'\n",
        "\n",
        "# Count images with typical image extensions\n",
        "image_count = len([f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "\n",
        "print(f\"Total number of images in the folder: {image_count}\")\n"
      ],
      "metadata": {
        "id": "jLtNPsBmQbFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Clear the new combined folder before copying\n",
        "combined_folder = \"/content/drive/MyDrive/combined_crackfolder\"\n",
        "\n",
        "if os.path.exists(combined_folder):\n",
        "    for file in os.listdir(combined_folder):\n",
        "        file_path = os.path.join(combined_folder, file)\n",
        "        if os.path.isfile(file_path):\n",
        "            os.remove(file_path)\n",
        "else:\n",
        "    os.makedirs(combined_folder)\n"
      ],
      "metadata": {
        "id": "eMc3CYCvWFXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Paths to your datasets\n",
        "folder1 = \"/content/extracted_cracks/crack_segmentation_dataset/images\"  # Replace with your actual path\n",
        "folder2 = \"/content/extracted_pothole3/dataset/dataset/classes/cracks/images\"  # Replace with your actual path\n",
        "combined_folder = \"/content/drive/MyDrive/combined_crackfolder\" # This will be created\n",
        "\n",
        "# Ensure combined folder exists\n",
        "os.makedirs(combined_folder, exist_ok=True)\n",
        "\n",
        "# Get all image filenames from both folders\n",
        "images1 = [f for f in os.listdir(folder1) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "images2 = [f for f in os.listdir(folder2) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "# Shuffle to ensure randomness (optional)\n",
        "random.shuffle(images1)\n",
        "random.shuffle(images2)\n",
        "\n",
        "# Select required number of images\n",
        "selected_images1 = images1[:1119]\n",
        "selected_images2 = images2[:166]\n",
        "\n",
        "# Copy images from folder1\n",
        "for i, image_name in enumerate(selected_images1):\n",
        "    src = os.path.join(folder1, image_name)\n",
        "    dst = os.path.join(combined_folder, f\"folder1_{i+1}_{image_name}\")\n",
        "    shutil.copy(src, dst)\n",
        "\n",
        "# Copy images from folder2\n",
        "for i, image_name in enumerate(selected_images2):\n",
        "    src = os.path.join(folder2, image_name)\n",
        "    dst = os.path.join(combined_folder, f\"folder2_{i+1}_{image_name}\")\n",
        "    shutil.copy(src, dst)\n",
        "\n",
        "print(\"✅ Dataset successfully combined!\")\n"
      ],
      "metadata": {
        "id": "tsasAc5dOOqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Define dataset paths\n",
        "binary_dataset_path = \"/content/binary_classification\"\n",
        "merged_pothole_dataset_path = \"/content/merged_pothole_dataset\"  # Ensure correct path\n",
        "crack_dataset_path = \"/content/drive/MyDrive/combined_crackfolder\"\n",
        "\n",
        "# Create binary classification folders\n",
        "pothole_binary_path = os.path.join(binary_dataset_path, \"pothole\")\n",
        "crack_binary_path = os.path.join(binary_dataset_path, \"crack\")\n",
        "\n",
        "os.makedirs(pothole_binary_path, exist_ok=True)\n",
        "os.makedirs(crack_binary_path, exist_ok=True)\n",
        "\n",
        "# Function to copy files from one folder to another\n",
        "def merge_datasets(source_path, destination_path):\n",
        "    if os.path.exists(source_path):\n",
        "        for file in os.listdir(source_path):\n",
        "            src = os.path.join(source_path, file)\n",
        "            dst = os.path.join(destination_path, file)\n",
        "            if os.path.isfile(src):\n",
        "                shutil.copy(src, dst)\n",
        "    else:\n",
        "        print(f\"❌ Error: Source folder {source_path} does not exist.\")\n",
        "\n",
        "# ✅ Step 1: Copy pothole images\n",
        "merge_datasets(merged_pothole_dataset_path, pothole_binary_path)\n",
        "\n",
        "# ✅ Step 2: Get total pothole images count\n",
        "total_pothole_images = len(os.listdir(pothole_binary_path))\n",
        "print(f\"✅ Total Pothole Images: {total_pothole_images}\")\n",
        "\n",
        "# ✅ Step 3: Clear old crack images before copying new ones\n",
        "for file in os.listdir(crack_binary_path):\n",
        "    os.remove(os.path.join(crack_binary_path, file))\n",
        "\n",
        "# ✅ Step 4: Get all crack images and select required amount\n",
        "all_crack_images = os.listdir(crack_dataset_path)\n",
        "\n",
        "# ✅ Step 5: Ensure we take exactly the number of pothole images\n",
        "if total_pothole_images > len(all_crack_images):\n",
        "    print(\"⚠️ Warning: Not enough crack images! Using available crack images instead.\")\n",
        "    total_pothole_images = len(all_crack_images)  # Adjust to available images\n",
        "\n",
        "selected_cracks = random.sample(all_crack_images, total_pothole_images)\n",
        "\n",
        "# ✅ Step 6: Copy selected crack images\n",
        "for file in selected_cracks:\n",
        "    src = os.path.join(crack_dataset_path, file)\n",
        "    dst = os.path.join(crack_binary_path, file)\n",
        "    shutil.copy(src, dst)\n",
        "\n",
        "# ✅ Step 7: Print final dataset count (Ensure equality)\n",
        "final_pothole_count = len(os.listdir(pothole_binary_path))\n",
        "final_crack_count = len(os.listdir(crack_binary_path))\n",
        "\n",
        "print(f\"✅ Final Counts → Potholes: {final_pothole_count} | Cracks: {final_crack_count}\")\n",
        "\n",
        "# ✅ Step 8: Double-check that numbers match\n",
        "if final_pothole_count == final_crack_count:\n",
        "    print(\"✅ Success! Both classes have equal images.\")\n",
        "else:\n",
        "    print(\"❌ Error: Counts do not match! Check for missing images.\")\n"
      ],
      "metadata": {
        "id": "ifkdd--wTlHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import timm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "b25_21R8VeKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Define your base directory\n",
        "base_dir = \"/content/binary_classification\"\n",
        "\n",
        "# Define class names (folders inside base_dir)\n",
        "classes = [\"pothole\", \"crack\"]\n",
        "\n",
        "# Create a class-to-label mapping\n",
        "class_map = {\"crack\": 1, \"pothole\": 0}\n",
        "\n",
        "# To store image paths and labels\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "# Collect images from all supported extensions\n",
        "extensions = ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']\n",
        "\n",
        "for class_name in classes:\n",
        "    folder_path = os.path.join(base_dir, class_name)\n",
        "\n",
        "    files = []\n",
        "    for ext in extensions:\n",
        "        files.extend(glob.glob(os.path.join(folder_path, ext)))\n",
        "\n",
        "    image_paths.extend(files)\n",
        "    labels.extend([class_map[class_name]] * len(files))\n",
        "\n",
        "# Check what's loaded\n",
        "print(f\"✅ Total images: {len(image_paths)}\")\n",
        "print(f\"🔍 First image path: {image_paths[0]}\")\n",
        "print(f\"🏷️ First label: {labels[0]}\")\n"
      ],
      "metadata": {
        "id": "aocU3gCFzDmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#removeeeeeeeee\n",
        "import os\n",
        "\n",
        "crack_path = \"/content/binary_classification/crack\"\n",
        "pothole_path = \"/content/binary_classification/pothole\"\n",
        "\n",
        "crack_images = [f for f in os.listdir(crack_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "pothole_images = [f for f in os.listdir(pothole_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "print(f\"🔍 Crack images: {len(crack_images)}\")\n",
        "print(f\"🔍 Pothole images: {len(pothole_images)}\")\n",
        "print(f\"📦 Total images: {len(crack_images) + len(pothole_images)}\")\n"
      ],
      "metadata": {
        "id": "P5CPdULc1w3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_idx, test_idx in sss.split(image_paths, labels):\n",
        "    train_paths = [image_paths[i] for i in train_idx]\n",
        "    train_labels = [labels[i] for i in train_idx]\n",
        "    test_paths = [image_paths[i] for i in test_idx]\n",
        "    test_labels = [labels[i] for i in test_idx]\n"
      ],
      "metadata": {
        "id": "waUwOux56Pv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "class PotholeCrackDataset(Dataset):\n",
        "    def __init__(self, paths, labels, transform):\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, self.labels[idx]\n",
        "\n",
        "train_loader = DataLoader(PotholeCrackDataset(train_paths, train_labels, train_transform), batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(PotholeCrackDataset(test_paths, test_labels, test_transform), batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "0tYje-Ha71UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=2)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
      ],
      "metadata": {
        "id": "EY5SsN6k74hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50  # Increase as needed\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), torch.tensor(labels).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "id": "tnLuFC9277xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), torch.tensor(labels).to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(all_labels, all_preds, target_names=['Pothole', 'Crack']))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=['Pothole', 'Crack'], yticklabels=['Pothole', 'Crack'])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-CHnoq0H8A7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:  # <-- use train_loader here\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the model on the training set: {train_accuracy:.2f}%')\n"
      ],
      "metadata": {
        "id": "8B76Ram-mns7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in test_loader:  # use your test_loader or validation_loader\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the model on the test set: {accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "cw7qDphvmWV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/content/drive/MyDrive/vit_pothole_crack_model.pt'\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(\"Model saved to:\", save_path)\n"
      ],
      "metadata": {
        "id": "Yz-CJB7S_8ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image(image_path, model, transform, class_names):\n",
        "    model.eval()\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    img = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img)\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        _, pred = torch.max(outputs, 1)\n",
        "\n",
        "    return class_names[pred.item()], probs[0][pred.item()].item()\n"
      ],
      "metadata": {
        "id": "ozAVFjFvAIYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import io\n",
        "import IPython.display as display\n",
        "\n",
        "# Upload image\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Display uploaded image\n",
        "for filename in uploaded.keys():\n",
        "    image_path = filename\n",
        "    print(\"Uploaded file:\", image_path)\n",
        "\n",
        "    # Open and display image\n",
        "    image = Image.open(io.BytesIO(uploaded[filename]))\n",
        "    display.display(image)\n",
        "\n"
      ],
      "metadata": {
        "id": "GE7cOr7TAO1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = [\"pothole\", \"crack\"]\n",
        "pred_class, confidence = predict_image(image_path, model, test_transform, class_names)\n",
        "print(f\"Predicted: {pred_class} ({confidence*100:.2f}% confidence)\")\n"
      ],
      "metadata": {
        "id": "zz90JWVWARS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D8EPCtV-Ara1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}